#  Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.
#  // SPDX-License-Identifier: BSD
import abc
import os
import tarfile
import tempfile
from pathlib import Path
from typing import Generator

import boto3
import click
import numpy as np
import prefixed
import prefixed as pr
import yaml
from PIL import Image
from joblib import parallel_config, Parallel, delayed, cpu_count
from tqdm import tqdm


class DataGenerator(abc.ABC):
    @abc.abstractmethod
    def generate(self, index: int, destdir: Path) -> bytes:
        pass


class ImageGenerator(DataGenerator):
    def __init__(self, width: int, height: int, img_format: str = "JPEG"):
        self.width = width
        self.height = height
        self.img_format = img_format

    def generate(self, index: int, destdir: Path):
        img = Image.fromarray(
            np.random.randint(
                0, high=256, size=(self.height, self.width, 3), dtype=np.uint8
            )
        )
        img_file: Path = destdir / f"image_{index}.{self.img_format}"
        img.save(img_file)


class Utils:
    @staticmethod
    def batch_files(
        files: list[Path], size_threshold: float
    ) -> Generator[list[Path], None, None]:
        group: list[Path] = []
        total = 0
        for file in files:
            new_total = total + file.stat().st_size
            if new_total > size_threshold:
                yield group
                group = [file]
                total = file.stat().st_size
            else:
                group.append(file)
                total = new_total
        if len(group) > 0:
            yield group

    @staticmethod
    def tar_files(name: str, workdir: Path, files: list[Path]) -> Path:
        tar_path: Path = workdir / f"{name}.tar"
        with tarfile.TarFile(tar_path, mode="w") as tarball:
            for file in files:
                tarball.add(file, arcname=file.name)
                os.remove(file)
        return tar_path

    @staticmethod
    def parse_resolution(
        ctx: click.Context, param: str, value: str
    ) -> tuple[int, int] | None:
        if value is not None:
            width, _, height = value.replace(" ", "").lower().partition("x")
            return int(width), int(height)

    @staticmethod
    def upload_to_s3(region: str, file: Path, bucket: str, prefix: str | None):
        s3_client = boto3.client("s3", region_name=region)
        s3_key_parts = []
        if prefix:
            s3_key_parts.append(prefix)
        s3_key_parts.append(os.path.basename(file))
        s3_key = "/".join(s3_key_parts)
        s3_client.upload_file(file, bucket, s3_key)

    @staticmethod
    def parse_human_readable_bytes(
        ctx: click.Context, param: str, value: str
    ) -> float | None:
        if value is not None:
            return prefixed.Float(value.rstrip("b").rstrip("B"))


@click.command(context_settings={"show_default": True})
@click.option(
    "-n",
    "--num-samples",
    type=pr.Float,
    default="1k",
    help="Number of samples to generate.  Can be supplied as an IEC or SI prefix. Eg: 1k, 2M."
    " Note: these are case-sensitive notations.",
)
@click.option(
    "--resolution",
    callback=Utils.parse_resolution,
    default="496x387",
    help="Resolution written in 'widthxheight' format",
)
@click.option(
    "--shard-size",
    callback=Utils.parse_human_readable_bytes,
    help="If supplied, the images are grouped into tar files of the given size."
    " Size can be supplied as an IEC or SI prefix. Eg: 16Mib, 4Kb, 1Gib."
    " Note: these are case-sensitive notations.",
)
@click.option(
    "--s3-bucket",
    type=str,
    help="S3 Bucket name. Note: Ensure the credentials are made available either through environment"
    " variables or a shared credentials file.",
    required=True,
)
@click.option(
    "--s3-prefix",
    type=str,
    help="Optional S3 Key prefix where the dataset will be uploaded. "
    "Note: a prefix will be autogenerated. eg: s3://<BUCKET>/1k_256x256_16Mib_sharded/",
)
@click.option(
    "--region",
    default="us-east-1",
    type=str,
    help="Region where the S3 bucket is hosted.",
)
def synthesize_dataset(
    num_samples: float,
    resolution: tuple[int, int],
    shard_size: float,
    s3_bucket: str,
    s3_prefix: str,
    region: str,
):
    """
    Synthesizes a dataset that will be used for benchmarking and uploads it to an S3 bucket.
    """
    with tempfile.TemporaryDirectory() as tempdir:
        tempdir = Path(tempdir)
        # TODO: parameterize the data generator to allow for creating other kinds of datasets(eg: text).
        generator = ImageGenerator(*resolution)
        # generate dataset locally
        with parallel_config(backend="multiprocessing", n_jobs=cpu_count()):
            s = int(num_samples)
            Parallel()(
                delayed(generator.generate)(i, tempdir)
                for i in tqdm(range(s), desc="Generating data")
            )
            if shard_size:
                files: list[Path] = [(tempdir / file) for file in os.listdir(tempdir)]
                Parallel()(
                    delayed(Utils.tar_files)(
                        f"train_shard_{batch_idx}", files=file_batch, workdir=tempdir
                    )
                    for (batch_idx, file_batch) in enumerate(
                        Utils.batch_files(files, shard_size)
                    )
                )
        # upload to s3
        with parallel_config(backend="threading", n_jobs=cpu_count()):
            files: list[Path] = [(tempdir / file) for file in os.listdir(tempdir)]
            disambiguator = (
                s3_prefix or f"{num_samples:.0h}_{resolution[0]}x{resolution[1]}_images"
            )
            if shard_size:
                disambiguator = disambiguator + f"_{shard_size:.0h}b_shards"
            fq_key = f"s3://{s3_bucket}/{disambiguator}/"
            Parallel()(
                delayed(Utils.upload_to_s3)(
                    region=region, file=file, bucket=s3_bucket, prefix=disambiguator
                )
                for file in tqdm(files, desc="Uploading to S3")
            )
            click.echo(f"Dataset uploaded to: {fq_key}")
        # generate hydra dataset config file
        dataset_cfg = {
            "prefix_uri": fq_key,
            "region": region,
            "sharding": bool(shard_size),
        }
        cfg_path = f"./configuration/dataset/{disambiguator}.yaml"
        with open(cfg_path, "w") as outfile:
            yaml.dump(dataset_cfg, outfile, default_flow_style=False)
            click.echo(f"Dataset Configuration created at: {cfg_path}")
        click.echo(
            f"Configure your experiment by setting the entry:\n\tdataset: {disambiguator}"
        )
        click.echo(
            "Alternatively, you can run specify it on the cmd-line when running the benchmark like so:"
        )
        click.echo(
            f"\tpython benchmark.py -m -cn <CONFIG-NAME> 'dataset={disambiguator}'"
        )


if __name__ == "__main__":
    synthesize_dataset()
