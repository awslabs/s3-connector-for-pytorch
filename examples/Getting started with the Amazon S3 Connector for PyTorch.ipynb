{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1191abe7-a33c-4d73-890e-64356629f2a4",
   "metadata": {},
   "source": [
    "# Getting started with the Amazon S3 Connector for PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8058c33b-76e8-4d2d-8bd1-f221f0fe9fb8",
   "metadata": {},
   "source": [
    "The Amazon S3 Connector for PyTorch delivers the highest throughput data access between Amazon S3 and a PyTorch training job, accelerating performance when interacting with machine learning training data and model checkpoints. Using the S3 Connector for PyTorch automatically optimizes performance when downloading training data from and writing checkpoints to Amazon S3, eliminating the need to write your own code to list S3 buckets and make concurrent requests.\n",
    "\n",
    "The S3 Connector for PyTorch provides implementations of PyTorch's [dataset primitives](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) that you can use to load training data from Amazon S3. It supports both [map-style datasets](https://pytorch.org/docs/stable/data.html#map-style-datasets) for random data access patterns and [iterable-style datasets](https://pytorch.org/docs/stable/data.html#iterable-style-datasets) for streaming sequential data access patterns. The S3 Connector for PyTorch also includes a checkpointing interface to save and load checkpoints directly to Amazon S3, without first saving to local storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9699cf87-5e40-4511-8e74-467b49a8e8f9",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "To install the S3 Connector for PyTorch, **[...installation instructions...]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9df48db-6f2f-4eb0-8db8-761b396bd1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3dataset\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a21bba-4e78-48a7-8237-2435822a3272",
   "metadata": {},
   "source": [
    "## Simple examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8294cb0f-1c51-445a-b20a-58ae99e00ac0",
   "metadata": {},
   "source": [
    "To illustrate how to use the S3 Connector for PyTorch, we've created a public Amazon S3 bucket with some example training data. [... talk about the dataset and the structure -- some images, from some friendly-licensed source. later we'll talk about WebDataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2d933-7b2e-4fe6-9a5e-c9c0231b2341",
   "metadata": {},
   "source": [
    "The simplest way to use the S3 Connector for PyTorch is to construct an `S3MapDataset`, a map-style dataset, by specifying an S3 URI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a98d6ee1-a827-4b98-a916-e4dea3460d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = s3dataset.S3MapDataset.from_prefix(\"s3://s3torchconnector-demo/images/\", region=\"us-west-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5397309-cd98-42db-8e48-f7667aae82d5",
   "metadata": {},
   "source": [
    "You can randomly access a map-style dataset by indexing into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "980b2877-d9f5-47c9-89bf-a9ac6a74b7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "list_objects; id=2 bucket=\"s3torchconnector-demo\" continued=false delimiter=\"\" max_keys=\"1000\" prefix=\"images/\"\n",
      "request failed request_type=Default http_status=-1 range=None duration=1.61000175s ttfb=None request_id=<unknown>\n",
      "meta request failed duration=1.611079583s error=ClientError(NoSigningCredentials)\n"
     ]
    },
    {
     "ename": "S3DatasetException",
     "evalue": "Client error: No signing credentials found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mS3DatasetException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO can you index only by integers, or by S3 URIs too?\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/Volumes/workspace/s3-connector-for-pytorch/s3dataset/src/s3dataset/s3map_dataset.py:36\u001b[0m, in \u001b[0;36mS3MapDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, i: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_objects\u001b[49m[i])\n",
      "File \u001b[0;32m/Volumes/workspace/s3-connector-for-pytorch/s3dataset/src/s3dataset/s3map_dataset.py:30\u001b[0m, in \u001b[0;36mS3MapDataset._dataset_objects\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dataset_objects\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[S3Object]:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_object_store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_object_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_dataset_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_object_store\n",
      "File \u001b[0;32m/Volumes/workspace/s3-connector-for-pytorch/s3dataset/src/s3dataset/_s3_bucket_iterable.py:56\u001b[0m, in \u001b[0;36m_PickleableListObjectStream.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ListObjectResult:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_list_stream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mS3DatasetException\u001b[0m: Client error: No signing credentials found"
     ]
    }
   ],
   "source": [
    "# TODO can you index only by integers, or by S3 URIs too?\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3de26-680d-4c34-b139-748dded47e62",
   "metadata": {},
   "source": [
    "**[what do you actually get back? explain `S3Reader`, it's a stream not a bytes]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6146855-132f-4008-a24f-6479d182dd67",
   "metadata": {},
   "source": [
    "Map-style datasets are also iterators, so you can iterate over them to retrieve every object in your S3 bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b775f16-d134-478d-ac17-0f67f378c05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "list_objects; id=1 bucket=\"s3torchconnector-demo\" continued=false delimiter=\"\" max_keys=\"1000\" prefix=\"images/\"\n",
      "request failed request_type=Default http_status=-1 range=None duration=1.431013042s ttfb=None request_id=<unknown>\n",
      "meta request failed duration=1.431248958s error=ClientError(NoSigningCredentials)\n"
     ]
    },
    {
     "ename": "S3DatasetException",
     "evalue": "Client error: No signing credentials found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mS3DatasetException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO visualize the output -- plot all the images or something\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/workspace/s3-connector-for-pytorch/s3dataset/src/s3dataset/s3map_dataset.py:36\u001b[0m, in \u001b[0;36mS3MapDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, i: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_objects\u001b[49m[i])\n",
      "File \u001b[0;32m/Volumes/workspace/s3-connector-for-pytorch/s3dataset/src/s3dataset/s3map_dataset.py:30\u001b[0m, in \u001b[0;36mS3MapDataset._dataset_objects\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dataset_objects\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[S3Object]:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_object_store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_object_store \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_dataset_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_client\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_object_store\n",
      "File \u001b[0;32m/Volumes/workspace/s3-connector-for-pytorch/s3dataset/src/s3dataset/_s3_bucket_iterable.py:56\u001b[0m, in \u001b[0;36m_PickleableListObjectStream.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ListObjectResult:\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_list_stream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mS3DatasetException\u001b[0m: Client error: No signing credentials found"
     ]
    }
   ],
   "source": [
    "# TODO visualize the output -- plot all the images or something\n",
    "for object in dataset:\n",
    "    print(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cfe729-985f-4795-8e45-5af7467b262b",
   "metadata": {},
   "source": [
    "## Working with `DataLoader`s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714792de-94b7-4cd5-b53b-0352bb649e06",
   "metadata": {},
   "source": [
    "While you can work directly with datasets, most PyTorch training loops will instead use a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader), a wrapper around a dataset that supports customizable ordering, automatic batching, and multi-process data loading.\n",
    "\n",
    "You can construct a `DataLoader` as a wrapper around an `S3MapDataset` or `S3IterableDataset`, passing in arguments such as the batch size you want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78786cf5-16db-480d-8559-808a00201cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3721e100-e6e6-4b57-a1d5-f525c971f9fd",
   "metadata": {},
   "source": [
    "A `DataLoader` is an iterator over *batches* of data samples (in this case, images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eadf485-93ce-44d0-be04-398ec994092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: iterate the loader, demonstrate the iterator yields batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b524ef02-831c-4cfc-8efa-43796d5319c2",
   "metadata": {},
   "source": [
    "### Multi-process data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba0c6e-50b0-451a-82f2-021c295ade85",
   "metadata": {},
   "source": [
    "To speed up data loading, you can configure a `DataLoader` to automatically spawn a number of worker processes and load data in parallel in each process, using the `num_workers` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e0e4346-2cf2-49cf-ba87-726d3c5447fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=4, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd95e48f-7aa7-42fb-a35f-d10533fd4ddc",
   "metadata": {},
   "source": [
    "Parallel data loading is especially important when loading training data from cloud storage services like Amazon S3, where the time to load each individual training sample may be high, but loading of many samples can happen in parallel. Multiple workers will give the best training throughput. We generally recommend setting `num_workers` to the number of vCPUs on your instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4b6a83-cb9a-4006-a912-3642c2eafa98",
   "metadata": {},
   "source": [
    "**Important**: When combining multi-process data loading with `S3IterableDataset`, by default each worker process will get its own replica of the dataset, and so each training sample will be duplicated `num_workers` times by the `DataLoader`. This is very likely not the behavior you want. **[we're working on it, link the github issue. in the meantime, show the torchdata version or something]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c32d72d-72d2-4896-8c0b-6da270f42701",
   "metadata": {},
   "source": [
    "## Training data formats for Amazon S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cb6350-827f-4d65-a27b-c842baf6ecd0",
   "metadata": {},
   "source": [
    "When storing training data in your Amazon S3 bucket, collecting training samples into preprocessed *shards* can improve the throughput of your training jobs as well as reducing the cost of loading the data. A shard is a single Amazon S3 object that contains many samples, rather than storing each individual training sample as a separate object. Collecting samples into larger shards allow your training jobs to make the best use of S3's elastic throughput by streaming the shards in their entirety, hiding the latency of individual requests. and lowering request costs.\n",
    "\n",
    "There are several ways to collect training data into shards. For textual data used to pre-train or fine-tune large language models, even simple sharding techniques like text files with one sample per line can be an effective sharding technique. For larger datasets or other data formats, consider open-source sharding formats like WebDataset or TensorFlow’s TFRecord.\n",
    "\n",
    "For example, you can use `S3IterableDataset` to stream training data stored in S3 in WebDataset format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "648443e0-19cd-4665-a999-5b6b1244544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO an example of WebDataset parsing and streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18799c33-d895-45a8-b492-2847f3a1bfca",
   "metadata": {},
   "source": [
    "Preprocessing your training samples into shards is also a good opportunity to optimize your training data format to reduce cost and improve training throughput. For example, you can pre-apply transformations like resizing, normalization, and tensor conversion to image or video datasets to avoid the overhead of these transformations during training. You can also compress the sharded objects before uploading them to Amazon S3 to reduce storage costs. Finally, sharded objects are more likely to be larger than the 128 KiB minimum size to be eligible for [S3 Intelligent Tiering](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/), which can further reduce storage costs for infrequently accessed training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4aea0a-34df-496a-ace0-ff5dd182e1dc",
   "metadata": {},
   "source": [
    "## Model checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2584146a-e5ac-4837-8cec-8de4de21b115",
   "metadata": {},
   "source": [
    "**[talk about checkpointing, torch.save]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aff8c87-de58-47c6-b424-cb0c16550f48",
   "metadata": {},
   "source": [
    "## An end-to-end example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f8a28-53d1-42da-9785-bd373bfcf5ff",
   "metadata": {},
   "source": [
    "**[train an actual model, using dataset for training data and checkpointing for saves. grab a simple example from HuggingFace or something.]**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
